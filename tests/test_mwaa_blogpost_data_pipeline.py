"""
Unit tests for MWAA Blogpost Data Pipeline DAG
Tests import compatibility, DAG structure, and task configuration
"""

import unittest
import sys
import os
from unittest.mock import patch, MagicMock
from datetime import datetime, timedelta

# Add the DAGs directory to the Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'assets', 'mwaa_dags', 'dags'))

class TestMWAABlogpostDataPipeline(unittest.TestCase):
    """Test cases for the MWAA Blogpost Data Pipeline DAG"""
    
    def setUp(self):
        """Set up test fixtures"""
        # Mock Airflow Variables to avoid dependency on actual Airflow environment
        self.variable_patcher = patch('airflow.models.Variable.get')
        self.mock_variable = self.variable_patcher.start()
        self.mock_variable.return_value = 'test-value'
        
    def tearDown(self):
        """Clean up after tests"""
        self.variable_patcher.stop()
        
    def test_dag_import(self):
        """Test that the DAG can be imported without errors"""
        try:
            from scripts.mwaa_blogpost_data_pipeline import dag
            self.assertIsNotNone(dag)
            print("‚úÖ DAG import successful")
        except ImportError as e:
            self.fail(f"Failed to import DAG: {e}")
        except Exception as e:
            self.fail(f"Unexpected error importing DAG: {e}")
    
    def test_custom_operator_import(self):
        """Test that the custom Glue operator can be imported"""
        try:
            from custom.glue_trigger_crawler_operator import GlueTriggerCrawlerOperator
            self.assertIsNotNone(GlueTriggerCrawlerOperator)
            print("‚úÖ Custom Glue operator import successful")
        except ImportError as e:
            self.fail(f"Failed to import custom operator: {e}")
        except Exception as e:
            self.fail(f"Unexpected error importing custom operator: {e}")
    
    def test_dag_structure(self):
        """Test DAG structure and configuration"""
        from scripts.mwaa_blogpost_data_pipeline import dag
        
        # Test DAG properties
        self.assertEqual(dag.dag_id, 'mwaa_blogpost_data_pipeline')
        self.assertEqual(dag.schedule, '@once')
        self.assertFalse(dag.catchup)
        self.assertEqual(set(dag.tags), {'emr', 'blogpost', 'mwaa'})
        print("‚úÖ DAG structure validation successful")
    
    def test_dag_tasks(self):
        """Test that all expected tasks are present in the DAG"""
        from scripts.mwaa_blogpost_data_pipeline import dag
        
        expected_tasks = [
            'download_movie_lens',
            'check_raw_s3_bucket',
            'create_emr_cluster',
            'run_glue_crawler',
            'query_athena_results'
        ]
        
        # Check base tasks
        for task_id in expected_tasks:
            self.assertIn(task_id, dag.task_dict, f"Task {task_id} not found in DAG")
        
        # Check dynamic EMR tasks (should have 3 steps)
        emr_step_tasks = [task_id for task_id in dag.task_dict.keys() if 'add_emr_spark_step_' in task_id]
        emr_sensor_tasks = [task_id for task_id in dag.task_dict.keys() if 'emr_spark_job_sensor_' in task_id]
        
        self.assertEqual(len(emr_step_tasks), 3, "Expected 3 EMR step tasks")
        self.assertEqual(len(emr_sensor_tasks), 3, "Expected 3 EMR sensor tasks")
        print("‚úÖ DAG tasks validation successful")
    
    def test_task_types(self):
        """Test that tasks are of the correct operator types"""
        from scripts.mwaa_blogpost_data_pipeline import dag
        from airflow.providers.standard.operators.python import PythonOperator
        from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
        from airflow.providers.amazon.aws.operators.emr import EmrCreateJobFlowOperator, EmrAddStepsOperator
        from airflow.providers.amazon.aws.sensors.emr import EmrStepSensor
        from airflow.providers.amazon.aws.operators.athena import AthenaOperator
        from custom.glue_trigger_crawler_operator import GlueTriggerCrawlerOperator
        
        # Test specific task types
        self.assertIsInstance(dag.get_task('download_movie_lens'), PythonOperator)
        self.assertIsInstance(dag.get_task('check_raw_s3_bucket'), S3KeySensor)
        self.assertIsInstance(dag.get_task('create_emr_cluster'), EmrCreateJobFlowOperator)
        self.assertIsInstance(dag.get_task('run_glue_crawler'), GlueTriggerCrawlerOperator)
        self.assertIsInstance(dag.get_task('query_athena_results'), AthenaOperator)
        
        # Test EMR step tasks
        self.assertIsInstance(dag.get_task('add_emr_spark_step_0'), EmrAddStepsOperator)
        self.assertIsInstance(dag.get_task('emr_spark_job_sensor_0'), EmrStepSensor)
        print("‚úÖ Task types validation successful")
    
    def test_default_args(self):
        """Test DAG default arguments"""
        from scripts.mwaa_blogpost_data_pipeline import default_args
        
        self.assertEqual(default_args['owner'], 'Airflow')
        self.assertIsInstance(default_args['start_date'], datetime)
        self.assertFalse(default_args['depends_on_past'])
        self.assertFalse(default_args['email_on_failure'])
        self.assertFalse(default_args['email_on_retry'])
        self.assertEqual(default_args['retries'], 1)
        print("‚úÖ Default args validation successful")
    
    @patch('requests.get')
    def test_download_dataset_function(self, mock_requests):
        """Test the download_dataset function"""
        from scripts.mwaa_blogpost_data_pipeline import download_dataset
        
        # Mock the requests response
        mock_response = MagicMock()
        mock_response.content = b'test zip content'
        mock_requests.return_value = mock_response
        
        # Mock context
        context = {
            'endpoint_path': 'http://test.com/test.zip',
            'templates_dict': {
                'bucket_name': 'test-bucket',
                'bucket_partition': '2023-01-01'
            }
        }
        
        # Mock S3Hook
        with patch('scripts.mwaa_blogpost_data_pipeline.S3Hook') as mock_s3_hook:
            mock_s3_instance = MagicMock()
            mock_s3_hook.return_value = mock_s3_instance
            
            # Mock zipfile to avoid actual zip processing
            with patch('scripts.mwaa_blogpost_data_pipeline.zipfile.ZipFile'):
                # This test mainly checks that the function can be called without import errors
                # Full functionality testing would require more complex mocking
                self.assertTrue(callable(download_dataset))
                print("‚úÖ Download dataset function validation successful")
    
    def test_spark_steps_configuration(self):
        """Test SPARK_STEPS configuration"""
        from scripts.mwaa_blogpost_data_pipeline import SPARK_STEPS
        
        self.assertEqual(len(SPARK_STEPS), 3)
        
        expected_names = ['process_movies_{{ ds_nodash }}', 'process_ratings_{{ ds_nodash }}', 'process_tags_{{ ds_nodash }}']
        actual_names = [step['Name'] for step in SPARK_STEPS]
        
        self.assertEqual(actual_names, expected_names)
        
        # Test that all steps have required structure
        for step in SPARK_STEPS:
            self.assertIn('Name', step)
            self.assertIn('ActionOnFailure', step)
            self.assertIn('HadoopJarStep', step)
            self.assertEqual(step['ActionOnFailure'], 'CONTINUE')
        print("‚úÖ Spark steps configuration validation successful")
    
    def test_emr_job_flow_overrides(self):
        """Test EMR job flow overrides configuration"""
        from scripts.mwaa_blogpost_data_pipeline import JOB_FLOW_OVERRIDES
        
        # Test required fields
        required_fields = ['Name', 'ReleaseLabel', 'Applications', 'Instances', 'JobFlowRole', 'ServiceRole']
        for field in required_fields:
            self.assertIn(field, JOB_FLOW_OVERRIDES)
        
        # Test specific values
        self.assertEqual(JOB_FLOW_OVERRIDES['Name'], 'mwaa-emr-cluster')
        self.assertEqual(JOB_FLOW_OVERRIDES['ReleaseLabel'], 'emr-6.4.0')
        
        # Test applications
        app_names = [app['Name'] for app in JOB_FLOW_OVERRIDES['Applications']]
        expected_apps = ['Hadoop', 'Spark', 'Hive']
        self.assertEqual(app_names, expected_apps)
        
        # Test instance configuration
        instances = JOB_FLOW_OVERRIDES['Instances']
        self.assertIn('InstanceGroups', instances)
        self.assertEqual(len(instances['InstanceGroups']), 2)  # MASTER and CORE
        
        # Test that EBS configuration is present
        for instance_group in instances['InstanceGroups']:
            self.assertIn('EbsConfiguration', instance_group)
            
        print("‚úÖ EMR job flow overrides validation successful")


class TestSyntaxAndImports(unittest.TestCase):
    """Test syntax and import compatibility"""
    
    def test_python_syntax(self):
        """Test that all Python files have valid syntax"""
        import ast
        
        files_to_test = [
            '../amazon-mwaa-workflow-demo/assets/mwaa_dags/dags/scripts/mwaa_blogpost_data_pipeline.py',
            '../amazon-mwaa-workflow-demo/assets/mwaa_dags/dags/custom/glue_trigger_crawler_operator.py'
        ]
        
        for file_path in files_to_test:
            if os.path.exists(file_path):
                with open(file_path, 'r') as f:
                    try:
                        ast.parse(f.read())
                        print(f"‚úÖ Syntax validation successful for {os.path.basename(file_path)}")
                    except SyntaxError as e:
                        self.fail(f"Syntax error in {file_path}: {e}")
            else:
                print(f"‚ö†Ô∏è  File not found: {file_path}")
    
    def test_airflow_imports(self):
        """Test that all Airflow imports are compatible with Airflow 3.0.6"""
        try:
            # Test core Airflow imports
            from airflow import DAG
            from airflow.providers.standard.operators.python import PythonOperator
            from airflow.models import Variable, BaseOperator
            
            # Test AWS provider imports
            from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
            from airflow.providers.amazon.aws.hooks.s3 import S3Hook
            from airflow.providers.amazon.aws.operators.emr import EmrAddStepsOperator, EmrCreateJobFlowOperator
            from airflow.providers.amazon.aws.sensors.emr import EmrStepSensor
            from airflow.providers.amazon.aws.operators.athena import AthenaOperator
            from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook
            
            print("‚úÖ All Airflow imports successful")
        except ImportError as e:
            self.fail(f"Import error: {e}")


if __name__ == '__main__':
    # Create a test suite
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    
    # Add test cases
    suite.addTest(loader.loadTestsFromTestCase(TestSyntaxAndImports))
    suite.addTest(loader.loadTestsFromTestCase(TestMWAABlogpostDataPipeline))
    
    # Run tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # Print summary
    print(f"\n{'='*50}")
    print(f"Tests run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")
    
    if result.failures:
        print("\nFailures:")
        for test, traceback in result.failures:
            print(f"- {test}: {traceback}")
    
    if result.errors:
        print("\nErrors:")
        for test, traceback in result.errors:
            print(f"- {test}: {traceback}")
    
    if result.wasSuccessful():
        print("üéâ All tests passed!")
    else:
        print("‚ùå Some tests failed!")
        sys.exit(1)
